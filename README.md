# Awesome Self-supervised Learning for Tabular Data
![Version](https://img.shields.io/badge/Version-1.0-lightgrey.svg) 
![LastUpdated](https://img.shields.io/badge/LastUpdated-2023.02-lightblue.svg)
![Topic](https://img.shields.io/badge/Topic-SSL%20for%20Tabular%20Data-pink?logo=github)

This repository contains the frontier research on **self-supervised learning** for tabular data which is a popular topic recently.<br>
This list is maintained by [Wei-Wei Du](https://wwweiwei.github.io/). (Actively keep updating)

## Papers
* [VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain (NeurIPS'20)](https://proceedings.neurips.cc/paper/2020/file/7d97667a3e056acab9aaf653807b4a03-Paper.pdf)
    *  [Supplementary](https://proceedings.neurips.cc/paper/2020/file/7d97667a3e056acab9aaf653807b4a03-Supplemental.pdf)
    *  [Code](https://github.com/jsyoon0823/VIME)
* [CORE: Self- and Semi-supervised Tabular Learning with COnditional REgularizations (NeurIPS'21)](https://sslneurips21.github.io/files/CameraReady/CORE_workshop.pdf)
* [SubTab: Subsetting Features of Tabular Data for Self-Supervised Representation Learning(NeurIPS'21)](https://arxiv.org/pdf/2110.04361.pdf)
    * [Supplementary](https://openreview.net/attachment?id=vrhNQ7aYSdr&name=supplementary_material)
    * [Code](https://github.com/AstraZeneca/SubTab)
* [SCARF: Self-Supervised Contrastive Learning using Random Feature Corruption (ICLR'22 Spotlight)](https://arxiv.org/pdf/2106.15147.pdf)
* [STab: Self-supervised Learning for Tabular Data (NeurIPS'22 Workshop on Table Representation Learning)](https://openreview.net/pdf?id=EfR55bFcrcI)
* [SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training (NurIPSâ€˜22 Workshop on Table Representation Learning)](https://arxiv.org/pdf/2106.01342.pdf)
    * [Code](https://github.com/somepago/saint)
* [TabNet: Attentive Interpretable Tabular Learning (AAAI'21)](https://arxiv.org/abs/1908.07442)
    * [Code](https://github.com/dreamquark-ai/tabnet)
* [TransTab: Learning Transferable Tabular Transformers Across Tables (NeurIPS'22)](https://arxiv.org/abs/2205.09328)
    * [Code](https://github.com/RyanWangZf/transtab)
    * [Blog](https://realsunlab.medium.com/transtab-learning-transferable-tabular-transformers-across-tables-1e34eec161b8)

#### Use correlation to capture relations between features
* [Self-Supervision Enhanced Feature Selection with Correlated Gates (ICLR'22)](https://openreview.net/pdf?id=oDFvtxzPOx)
    * [Code](https://github.com/chl8856/SEFS)
* [Local Contrastive Feature Learning for Tabular Data (CIKM'22)](https://dl.acm.org/doi/pdf/10.1145/3511808.3557630)

#### Use pretrained language model to learn better representation
* [TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data (ACL'20)](https://arxiv.org/abs/2005.08314)
* [LIFT: Language-Interfaced Fine-Tuning for Non-language Machine Learning Tasks (NeurIPS'22)](https://arxiv.org/pdf/2206.06565.pdf)
    * [Code](https://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning)
* [Pretrained Transformers As Universal Computation Engines](https://arxiv.org/pdf/2103.05247.pdf)
    * [Code](https://github.com/kzl/universal-computation)

## Tutorials
* [Self-Supervised Learning: Self-Prediction and Contrastive Learning (NeurIPS'21)](https://neurips.cc/virtual/2021/tutorial/21895)
